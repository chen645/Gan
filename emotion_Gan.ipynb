{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in c:\\users\\superuser\\.conda\\envs\\tgpu\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\superuser\\.conda\\envs\\tgpu\\lib\\site-packages (from tensorflow-addons) (2.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow-addons\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import librosa\n",
    "import time\n",
    "from os import path\n",
    "import os\n",
    "import imageio\n",
    "from scipy.io.wavfile import write\n",
    "from zipfile import ZipFile\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import h5py\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def makedir(path,file):\n",
    "    url = os.path.join(path,file)\n",
    "    if os.path.exists(url):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(url)\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "filename=  'emotion_speech/true_emotion_speech/*/*.wav'\n",
    "filepath = glob(filename)\n",
    "count  =len(filepath)\n",
    "for i, path in enumerate(filepath):\n",
    "    y, sr = librosa.load(path,sr=16000)\n",
    "    D = np.abs(librosa.stft(y))\n",
    "    amp_max = np.amax(D)\n",
    "    x1 = librosa.amplitude_to_db(D, ref=np.max)\n",
    "    max_x1 = np.amax(x1)\n",
    "    min_x1 = np.amin(x1)\n",
    "    root = 'cycleGAN'\n",
    "    path2 = path.split('\\\\')[-2]\n",
    "    root_path=makedir(root,path2)\n",
    "    true_path = root_path+\"\\\\\"+path.split('\\\\')[-1][:-4]+'.jpg'\n",
    "    diff = max_x1 - min_x1\n",
    "    imageio.imwrite(true_path, x1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "convert_data() missing 2 required positional arguments: 'w_f' and 'w_w_f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0a3b53b507cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mspecto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"spec.png\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: convert_data() missing 2 required positional arguments: 'w_f' and 'w_w_f'"
     ]
    }
   ],
   "source": [
    "def read_audio_from_filename(filename):\n",
    "    audio, sr = librosa.load(filename,sr=16000)\n",
    "    return audio,sr\n",
    "\n",
    "def write_audio_to_file(audio,name):\n",
    "    audio1 = write(name,16000,audio)\n",
    "    return audio1\n",
    "\n",
    "def make_spectrogram(audio,sr):\n",
    "    spec=librosa.stft(audio)\n",
    "    \n",
    "    liborsa.display.specshow(librosa.power_to_db(spec,ref=np.max))\n",
    "    a = np.abs(spec)**2\n",
    "    a = a/np.amax(a)\n",
    "    a = 10*log10(a)\n",
    "    a = a*255/np.amax(a)\n",
    "    plt.imshow(a)\n",
    "    plt.show()\n",
    "    return a\n",
    "\n",
    "def make_audio(spec):\n",
    "    res = librosa.griffinlim(spec)\n",
    "    return res\n",
    "\n",
    "def convert_data(w_f,w_w_f):\n",
    "    audio,sr = read_audio_from_filename(w_f)\n",
    "    spec = make_spectrogram(audio,sr)\n",
    "    res = make_audio(spec)\n",
    "    result = write_audio_to_file(res,w_w_f)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "specto = convert_data()\n",
    "im = cv2.imread(\"spec.png\",0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c74773eb54a478c8fa79c8a8aeb4a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc2c9e7fd3847e6be2d32cf3feb7187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f711e8927fbf464b9a3363a0d5b4a7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c2760fceb94fd294829ce917f77da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35622cb3bdb4266aede34f961c9213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c058f09c151b46f9ad66afe9f3492f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03caf9b9cb3f4b17a08ff8c24f79e209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "thesis/angry2disgust\n"
     ]
    }
   ],
   "source": [
    "class_map = {'angry':0,'disgust':1, 'fearful':2,'happy':3,'neautral':4,'sad':5,'surprised':6}\n",
    "IMG_SIZE=40\n",
    "def database(label):\n",
    "    \n",
    "    filename = 'cycleGAN\\\\'+label+'\\\\*.jpg'\n",
    "    img_paths = glob(filename)\n",
    "    data_count = len(img_paths)\n",
    "    X = np.zeros((data_count, IMG_SIZE, IMG_SIZE))\n",
    "    label = np.zeros((data_count, ))\n",
    "    for i, path in tqdm(enumerate(img_paths)):\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        img_resize = cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "        img_resize = (img_resize - 127.5) / 127.5\n",
    "        X[i] = img_resize\n",
    "        cls = path.split('\\\\')[-2]\n",
    "        \n",
    "        label[i] = class_map[cls]\n",
    "    return X,label\n",
    "\n",
    "X_angry,label_angry = database(\"angry\")\n",
    "X_disgust,label_disgust = database('disgust')\n",
    "X_fearful,label_fearful = database('fearful')\n",
    "X_happy,label_happy = database('happy')\n",
    "X_neautral,label_neautral = database('neautral')\n",
    "X_sad, label_sad = database('sad')\n",
    "X_surprised,label_surprised = database('surprised')\n",
    "emo1='angry'\n",
    "emo2='disgust'\n",
    "filename1='thesis/'+emo1+'2'+emo2\n",
    "filename2='thesis/'+emo2+'2'+emo1\n",
    "print(filename1)\n",
    "# save npz file\n",
    "np.savez_compressed(filename1,X_angry,X_disgust)\n",
    "np.savez_compressed(filename2,X_disgust,X_angry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "  path = os.path.join(base, name)\n",
    "  if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "  return path\n",
    "\n",
    "# change the following lines\n",
    "main_dir = \"home\"\n",
    "domain_a = \"angry\"\n",
    "domain_b = \"neautral\"\n",
    "name_of_npz = domain_a+\"2\"+domain_b+\".npz\"\n",
    "path_to_npz = \"/home/\"+name_of_npz\n",
    "\n",
    "fft_len = 512\n",
    "spec_dim = 40\n",
    "hop_length = 256\n",
    "\n",
    "path_to_logs = mkdir(main_dir,\"logs\")\n",
    "path_to_log = mkdir(path_to_logs,domain_a+'-'+domain_b+'_'+str(int(time.time())))\n",
    "assert path_to_npz.split(\"/\")[-1][:-4] == domain_a+\"2\"+domain_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7a7bd9ad5833>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mX2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loaded'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;31m# define input shape based on the loaded dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[0mimage_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "def define_discriminator(image_shape):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# source image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# C64\n",
    "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C128\n",
    "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C256\n",
    "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# C512\n",
    "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# second last output layer\n",
    "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\n",
    "\td = LeakyReLU(alpha=0.2)(d)\n",
    "\t# patch output\n",
    "\tpatch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, patch_out)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n",
    "\tplot_model(model, to_file='discriminator_model.png', show_shapes=True, show_layer_names=True)\n",
    "\treturn model\n",
    "\n",
    "# generator a resnet block\n",
    "def resnet_block(n_filters, input_layer):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# first layer convolutional layer\n",
    "\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n",
    "\n",
    "\tg = Activation('relu')(g)\n",
    "\t# second convolutional layer\n",
    "\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n",
    "\n",
    "\t# concatenate merge channel-wise with input layer\n",
    "\tg = Concatenate()([g, input_layer])\n",
    "\treturn g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape, n_resnet=9):\n",
    "\t# weight initialization\n",
    "\tinit = RandomNormal(stddev=0.02)\n",
    "\t# image input\n",
    "\tin_image = Input(shape=image_shape)\n",
    "\t# c7s1-64\n",
    "\tg = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n",
    "\n",
    "\tg = Activation('relu')(g)\n",
    "\t# d128\n",
    "\tg = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\n",
    "\tg = Activation('relu')(g)\n",
    "\t# d256\n",
    "\tg = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\n",
    "\tg = Activation('relu')(g)\n",
    "\t# R256\n",
    "\tfor _ in range(n_resnet):\n",
    "\t\tg = resnet_block(256, g)\n",
    "\t# u128\n",
    "\tg = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\n",
    "\tg = Activation('relu')(g)\n",
    "\t# u64\n",
    "\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n",
    "\n",
    "\tg = Activation('relu')(g)\n",
    "\t# c7s1-3\n",
    "\tg = Conv2D(1, (7,7), padding='same', kernel_initializer=init)(g)\n",
    "\n",
    "\tout_image = Activation('tanh')(g)\n",
    "\t# define model\n",
    "\tmodel = Model(in_image, out_image)\n",
    "\tplot_model(model, to_file='generator_model.png', show_shapes=True, show_layer_names=True)\n",
    "\treturn model\n",
    "\n",
    "# define a composite model for updating generators by adversarial and cycle loss\n",
    "def define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n",
    "\t# ensure the model we're updating is trainable\n",
    "\tg_model_1.trainable = True\n",
    "\t# mark discriminator as not trainable\n",
    "\td_model.trainable = False\n",
    "\t# mark other generator model as not trainable\n",
    "\tg_model_2.trainable = False\n",
    "\t# discriminator element\n",
    "\tinput_gen = Input(shape=image_shape)\n",
    "\tgen1_out = g_model_1(input_gen)\n",
    "\toutput_d = d_model(gen1_out)\n",
    "\t# identity element\n",
    "\tinput_id = Input(shape=image_shape)\n",
    "\toutput_id = g_model_1(input_id)\n",
    "\t# forward cycle\n",
    "\toutput_f = g_model_2(gen1_out)\n",
    "\t# backward cycle\n",
    "\tgen2_out = g_model_2(input_id)\n",
    "\toutput_b = g_model_1(gen2_out)\n",
    "\t# define model graph\n",
    "\tmodel = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "\t# define optimization algorithm configuration\n",
    "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\t# compile model with weighting of least squares loss and L1 loss\n",
    "\tmodel.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "\tplot_model(model, to_file='composite_model.png', show_shapes=True, show_layer_names=True)\n",
    "\treturn model\n",
    "\n",
    "# load and prepare training images\n",
    "def load_real_samples(filename):\n",
    "\t# load the dataset\n",
    "\tdata = np.load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\t# scale from [0,255] to [-1,1]\n",
    "\tX1 = (X1 - 127.5) / 127.5\n",
    "\tX2 = (X2 - 127.5) / 127.5\n",
    "\treturn [X1, X2]\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "\n",
    "dataset = reshape((image_shape[0], image_shape[1], 1))\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, dataset.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = np.ones((n_samples, patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(dataset)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = np.zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# save the generator models to file\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "\t# save the first generator model\n",
    "\tfilename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n",
    "\tg_model_AtoB.save(filename1)\n",
    "\t# save the second generator model\n",
    "\tfilename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n",
    "\tg_model_BtoA.save(filename2)\n",
    "\tprint('>Saved: %s and %s' % (filename1, filename2))\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, trainX, name, n_samples=5):\n",
    "\t# select a sample of input images\n",
    "\tX_in, _ = generate_real_samples(trainX, n_samples, 0)\n",
    "\t# generate translated images\n",
    "\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_in = (X_in + 1) / 2.0\n",
    "\tX_out = (X_out + 1) / 2.0\n",
    "\t# plot real images\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(2, n_samples, 1 + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_in[i].reshape(167,1025), cmap='gray')\n",
    "\t# plot translated image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tplt.subplot(2, n_samples, 1 + n_samples + i)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.imshow(X_out[i].reshape(167,1025), cmap='gray')\n",
    "\t# save plot to file\n",
    "\tfilename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "\tplt.savefig(filename1)\n",
    "\tplt.close()\n",
    "\n",
    "# update image pool for fake images\n",
    "def update_image_pool(pool, images, max_size=50):\n",
    "\tselected = list()\n",
    "\tfor image in images:\n",
    "\t\tif len(pool) < max_size:\n",
    "\t\t\t# stock the pool\n",
    "\t\t\tpool.append(image)\n",
    "\t\t\tselected.append(image)\n",
    "\t\telif np.random.random() < 0.5:\n",
    "\t\t\t# use image, but don't add it to the pool\n",
    "\t\t\tselected.append(image)\n",
    "\t\telse:\n",
    "\t\t\t# replace an existing image and use replaced image\n",
    "\t\t\tix = np.random.randint(0, len(pool))\n",
    "\t\t\tselected.append(pool[ix])\n",
    "\t\t\tpool[ix] = image\n",
    "\treturn np.asarray(selected)\n",
    "\n",
    "# train cyclegan models\n",
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n",
    "\t# define properties of the training run\n",
    "\tn_epochs, n_batch, = 50, 1\n",
    "\t# determine the output square shape of the discriminator\n",
    "\tn_patch = d_model_A.output_shape[1]\n",
    "\t# unpack dataset\n",
    "\ttrainA, trainB = dataset\n",
    "\t# prepare image pool for fakes\n",
    "\tpoolA, poolB = list(), list()\n",
    "\t# calculate the number of batches per training epoch\n",
    "\tbat_per_epo = int(len(trainA) / n_batch)\n",
    "\t# calculate the number of training iterations\n",
    "\tn_steps = bat_per_epo * n_epochs\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "\t\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n",
    "\t\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n",
    "\t\t# generate a batch of fake samples\n",
    "\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "\t\t# update fakes from pool\n",
    "\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "\t\t# update generator B->A via adversarial and cycle loss\n",
    "\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "\t\t# update discriminator for A -> [real/fake]\n",
    "\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "\t\t# update generator A->B via adversarial and cycle loss\n",
    "\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "\t\t# update discriminator for B -> [real/fake]\n",
    "\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "\t\t# summarize performance\n",
    "\t\tprint('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "\t\t# evaluate the model performance every so often\n",
    "\t\tif (i+1) % (bat_per_epo * 1) == 0:\n",
    "\t\t\t# plot A->B translation\n",
    "\t\t\tsummarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n",
    "\t\t\t# plot B->A translation\n",
    "\t\t\tsummarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n",
    "\t\tif (i+1) % (bat_per_epo * 5) == 0:\n",
    "\t\t\t# save the models\n",
    "\t\t\tsave_models(i, g_model_AtoB, g_model_BtoA)\n",
    "\n",
    "# load image data\n",
    "dataset = load_real_samples('thesis/angry2disgust.npz')\n",
    "#dataset[0] = dataset[0][:int(0.25*len(dataset[0]))]\n",
    "#dataset[1] = dataset[1][:int(0.20*len(dataset[1]))]\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "# generator: A -> B\n",
    "g_model_AtoB = define_generator(image_shape)\n",
    "# generator: B -> A\n",
    "g_model_BtoA = define_generator(image_shape)\n",
    "# discriminator: A -> [real/fake]\n",
    "d_model_A = define_discriminator(image_shape)\n",
    "# discriminator: B -> [real/fake]\n",
    "d_model_B = define_discriminator(image_shape)\n",
    "# composite: A -> B -> [real/fake, A]\n",
    "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
    "# composite: B -> A -> [real/fake, B]\n",
    "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
    "# train models\n",
    "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (301, 40, 40) (154, 40, 40)\n"
     ]
    }
   ],
   "source": [
    "spec_dim=40\n",
    "# load image data\n",
    "\n",
    "def load_real_samples(filename):\n",
    "\t# load the dataset\n",
    "\tdata = np.load(filename)\n",
    "\t# unpack arrays\n",
    "\tX1, X2 = data['arr_0'], data['arr_1']\n",
    "\treturn [X1, X2]\n",
    "dataset = load_real_samples('thesis/angry2disgust.npz')\n",
    "#dataset[0] = dataset[0][:int(0.25*len(dataset[0]))]\n",
    "#dataset[1] = dataset[1][:int(0.20*len(dataset[1]))]\n",
    "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
    "# define input shape based on the loaded dataset\n",
    "image_shape = dataset[0].shape[1:]\n",
    "\n",
    "image_shape = (spec_dim,spec_dim,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a1ed3abadaa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# generator: A -> B\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mg_model_AtoB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# generator: B -> A\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mg_model_BtoA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# discriminator: A -> [real/fake]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7a7bd9ad5833>\u001b[0m in \u001b[0;36mdefine_generator\u001b[1;34m(image_shape, n_resnet)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m# define model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'generator_model.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_model' is not defined"
     ]
    }
   ],
   "source": [
    "# generator: A -> B\n",
    "g_model_AtoB = define_generator(image_shape)\n",
    "# generator: B -> A\n",
    "g_model_BtoA = define_generator(image_shape)\n",
    "# discriminator: A -> [real/fake]\n",
    "d_model_A = define_discriminator(image_shape)\n",
    "# discriminator: B -> [real/fake]\n",
    "d_model_B = define_discriminator(image_shape)\n",
    "# composite: A -> B -> [real/fake, A]\n",
    "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n",
    "# composite: B -> A -> [real/fake, B]\n",
    "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n",
    "\n",
    "# train models\n",
    "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
